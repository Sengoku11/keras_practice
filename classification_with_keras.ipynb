{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits recognition with Keras\n",
    "\n",
    "I will build a conventional Neural Network that recognizes handwriting digits using Keras API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 9369650751326744231, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6662668288\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 14638960966908209854\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow-GPU is working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images of digits written by high school students and employees of the United States Census Bureau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a060baccc8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "With conventional neural networks I have to vectorize images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels)\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels)\n",
    "X_val = X_val.reshape(X_val.shape[0], num_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / X_train.max()\n",
    "X_test = X_test / X_test.max()\n",
    "X_val = X_val / X_val.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Keras have removed f1 score I will create a custom one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, activation=\"relu\", input_shape=(num_pixels, )))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dense(y_train.shape[1], activation=\"softmax\"))    \n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy', f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.2000 - accuracy: 0.9393 - f1: 0.9362 - val_loss: 0.1242 - val_accuracy: 0.9616 - val_f1: 0.9617\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.0835 - accuracy: 0.9741 - f1: 0.9742 - val_loss: 0.1208 - val_accuracy: 0.9608 - val_f1: 0.9615\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.0573 - accuracy: 0.9817 - f1: 0.9817 - val_loss: 0.0939 - val_accuracy: 0.9721 - val_f1: 0.9726\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.0416 - accuracy: 0.9864 - f1: 0.9865 - val_loss: 0.0887 - val_accuracy: 0.9734 - val_f1: 0.9741\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.0336 - accuracy: 0.9889 - f1: 0.9888 - val_loss: 0.1190 - val_accuracy: 0.9701 - val_f1: 0.9702\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.0267 - accuracy: 0.9915 - f1: 0.9916 - val_loss: 0.1171 - val_accuracy: 0.9703 - val_f1: 0.9704\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.0236 - accuracy: 0.9922 - f1: 0.9922 - val_loss: 0.1197 - val_accuracy: 0.9732 - val_f1: 0.9733\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.0197 - accuracy: 0.9935 - f1: 0.9935 - val_loss: 0.1115 - val_accuracy: 0.9736 - val_f1: 0.9740\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.0170 - accuracy: 0.9944 - f1: 0.9944 - val_loss: 0.1257 - val_accuracy: 0.9733 - val_f1: 0.9735\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.0159 - accuracy: 0.9946 - f1: 0.9947 - val_loss: 0.1315 - val_accuracy: 0.9742 - val_f1: 0.9746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2a009e45a88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = classification_model()\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.97%\n",
      "F1: 98.01%\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1_scores = scores[1], scores[2]\n",
    "\n",
    "print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
    "print('F1: {:.2f}%'.format(f1_scores*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist_nn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
